# -*- coding: utf-8 -*-
"""web-scraping-box-office-data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uYhWXOe5Vnqmlpq0LrineU_J2FAflYl8

# Scraping Box Office Mojo Data Using Python
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from bs4 import BeautifulSoup
import requests
from datetime import date

"""Web scraping is simply crawling the html and javascript of a webpage to extract the details we want to use as data. The most complicated part of this is inspecting the webpage source code to determine what to grab and what to ignore.

In this demo I'll demonstrate how to create a dataset from the weekend performance page of Box Office Mojo, a great datasource for box office performance data. The associated dataset that is created and updated from this notebook can be found here:

### [U.S. Weekend Box Office Summary](https://www.kaggle.com/datasets/jonbown/weekend-box-office-summaries)

* Check if website allows Scraping
* Determine the URL
* Make Request
* Use Beautiful Soup
* Inspection
* Testing
* Create Write Function

<h1 style='background:#2da6c4;border:0; color:black;
    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);
    transform: rotateX(10deg);
    '><center style='color: white;'>1. Check if website allows scraping</center></h1>

Take the root of the url, in this case https://www.boxofficemojo.com and add '/robots.txt' to the end. This will come up with a page that shows what type of web scraping is allowed or disallowed. Thankfully Box Office Mojo allows all.

<a id="2"></a>
<h1 style='background:#2da6c4;border:0; color:black;
    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);
    transform: rotateX(10deg);
    '><center style='color: white;'>2. Determine the URL</center></h1>
"""

# Request to website and download HTML contents
url='https://www.boxofficemojo.com/weekend/by-year/2021/'
#you may need to play around with the different urls to find the ones that take parameters

"""<a id="3"></a>
<h1 style='background:#2da6c4;border:0; color:black;
    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);
    transform: rotateX(10deg);
    '><center style='color: white;'>3. Make Request</center></h1>
"""

req=requests.get(url)
content=req.text

"""<a id="4"></a>
<h1 style='background:#2da6c4;border:0; color:black;
    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);
    transform: rotateX(10deg);
    '><center style='color: white;'>4. Use Beautiful Soup</center></h1>
"""

soup=BeautifulSoup(content)

"""<a id="5"></a>
<h1 style='background:#2da6c4;border:0; color:black;
    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);
    transform: rotateX(10deg);
    '><center style='color: white;'>5. Inspect page data to determine out to set up the scraping algorithm</center></h1>

> Generally tabular data that is visible on the page will be put into 'tr' tags. With some of the code below I am exploring edge cases in the ouput of the page. When there is a special occasion for the weekend, in this case thanksgiving, there is a different format and structure that is displayed.
"""

rows=soup.findAll('tr')

data = rows[8].findAll('td')

data

#Date when there is a special occasion listed
data[0].findAll('a')[0].text

#Special occasion that is listed
data[0].findAll('span')[0].text

data[0].findAll('span')

"""<a id="6"></a>
<h1 style='background:#2da6c4;border:0; color:black;
    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);
    transform: rotateX(10deg);
    '><center style='color: white;'>6. Test data construction</center></h1>
"""

appended_data = []
for row in rows:
    data_row = {}
    data = row.findAll('td')
    if len(data) == 0:
        continue
    if len(data[0].findAll('span')) > 0:
        #special weekend
        data_row['occasion'] = data[0].findAll('span')[0].text
        data_row['date'] = data[0].findAll('a')[0].text
    else:
        #normal weekend
        data_row['occasion'] = ""
        data_row['date'] = data[0].text
    data_row['top10_gross'] = data[1].text
    data_row['top10_wow_change'] = data[2].text
    data_row['overall_gross'] = data[3].text
    data_row['overall_wow_change'] = data[4].text
    data_row['num_releases'] = data[5].text
    data_row['top_release'] = data[6].text
    data_row['week_no'] = data[10].text
    appended_data.append(data_row)
weekend_data = pd.DataFrame(appended_data, columns = ['date','occasion', 'top10_gross', 'top10_wow_change', 'overall_gross', 'overall_wow_change', 'num_releases', 'top_release', 'week_no'])

weekend_data.head()

"""<a id="7"></a>
<h1 style='background:#2da6c4;border:0; color:black;
    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);
    transform: rotateX(10deg);
    '><center style='color: white;'>7. Create write function</center></h1>

### Make sure to write to kaggle working directory
"""

def scrape_weekend_summary(year):
    url=f'https://www.boxofficemojo.com/weekend/by-year/{year}/'
    req=requests.get(url)
    content=req.text
    soup=BeautifulSoup(content)
    rows=soup.findAll('tr')
    appended_data = []
    for row in rows:
        data_row = {}
        data = row.findAll('td')
        if len(data) == 0:
            continue
        if len(data[0].findAll('span')) > 0:
        #special weekend
            data_row['occasion'] = data[0].findAll('span')[0].text
            data_row['date'] = data[0].findAll('a')[0].text
        else:
        #normal weekend
            data_row['occasion'] = ""
            data_row['date'] = data[0].text
        data_row['top10_gross'] = data[1].text
        data_row['top10_wow_change'] = data[2].text
        data_row['overall_gross'] = data[3].text
        data_row['overall_wow_change'] = data[4].text
        data_row['num_releases'] = data[5].text
        data_row['top_release'] = data[6].text
        data_row['week_no'] = data[10].text
        appended_data.append(data_row)
    weekend_data = pd.DataFrame(appended_data, columns = ['date', 'occasion', 'top10_gross', 'top10_wow_change', 'overall_gross', 'overall_wow_change', 'num_releases', 'top_release', 'week_no'])
    weekend_data.to_csv(f'/kaggle/working/weekend_summary_{year}.csv', index=False)

#Test on one year
#scrape_weekend_summary('2022')

#Get current year
todays_date = date.today()
current_year = todays_date.year

years = range(1977, current_year+1)

for year in years:
    print(year)
    scrape_weekend_summary(year)